Data:
Data refers to raw facts and figures. It can be in the form of numbers, text, images, or any other format. 
Data by itself is not meaningful, it becomes information when processed and organized.

Information:
Information is the result of processing and organizing data in a meaningful way. It provides context, relevance, and purpose, making it useful for decision-making.

Database:
A database is a structured collection of data that is organized (typically using tables with predefined columns and data types) and stored in a way that allows for efficient retrieval and manipulation.

Database Management System (DBMS):
A Database Management System is software that enables the creation, maintenance, and use of databases. It provides an interface for interacting with the database, ensuring data integrity, security, and efficient retrieval. Examples of DBMS include MySQL, Oracle, Microsoft SQL Server, and PostgreSQL.

Database System:
A Database System is a combination of a database and its DBMS along with hardware. It includes the database(stored data), DBMS(software that manages and provides access to that data.), and the hardware upon which this DBMS is installed or running.

Applications (Use-Cases) of DBMS:
Web Applications, Mobile Applications, E-commerce applications, Enterprise applications, Social networking Applications, Gaming Applications, Data science, Machine Learning etc



History of Data (File-Based Storage):
File-based storage or flat file storage, involves storing data in files on a file system. 

Why not use file-based? / Limitations:

Data Redundancy:
In file storage, data redundancy can be a common issue. If the same data needs to be stored in multiple files, any updates or changes to that data may lead to inconsistencies.

Limited Query Capabilities:
Retrieving specific pieces of data often involves reading through entire files, which can be inefficient and time-consuming, especially as the volume of data increases.

Data Isolation:
Each file in a file storage system is typically independent, making it challenging to establish relationships or connections between different sets of data. This can hinder the ability to perform retrieve related information.

Concurrency Issues:
File systems may not handle concurrent access well, leading to potential conflicts when multiple users or processes attempt to access or modify the same file simultaneously. This lack of concurrency control can result in data inconsistencies.

Limited Security Features:
File systems may lack robust security features compared to DBMS. Implementing access controls, authentication, and encryption can be more challenging in a file storage environment.

Scalability Challenges:
File storage systems might face scalability challenges as the volume of data increases. Managing a large number of files can become unwieldy, and optimizing performance may become difficult.

Data Integrity Concerns:
Maintaining data integrity in file storage systems can be challenging. There is a higher risk of data corruption or inconsistencies, especially if updates are not managed carefully.

Limited Support for Transactions:
File storage systems may not provide built-in support for transactions, making it harder to ensure atomicity, consistency, isolation, and durability (ACID properties) in data operations.

In summary, while file storage may be suitable for simple and small-scale applications, a DBMS offers a more robust, efficient, and secure solution for managing and accessing data, especially in environments with complex data relationships and a need for concurrent data access.


Solution: DBMS Advantages:

Transactions & ACID Properties: 
ACID (Atomicity, Consistency, Isolation, Durability) represents a set of properties ensuring the reliability and consistency of database transactions by guaranteeing that they are atomic, maintain data consistency, operate in isolation, and endure system failures.

Normalization:
DBMS minimizes data redundancy through normalization techniques, reducing the chances of inconsistencies and making updates or changes more manageable.

Data Integrity:
DBMS enforces data integrity through constraints (e.g., unique keys, foreign keys, and check constraints). This ensures that the data in the database is accurate and consistent.

Data Modeling & Relationships:
DBMS allows the use of data models and schema definitions, providing a clear structure for the data. DBMS also supports the establishment of relationships between different tables (entities) using keys. This relational structure allows for more efficient querying and retrieval of related data compared to flat file storage.

Concurrency Control:
DBMS provides mechanisms for handling multiple users accessing the database simultaneously while maintaining consistency. This is essential for applications with concurrent user activity.

Query Language:
DBMS uses a standardized query language like SQL, making it easier to retrieve, update, and manipulate data. This simplifies the process of interacting with the stored information.

Security:
DBMS offers robust security features, including user authentication, authorization, and encryption. File systems may lack these built-in security measures, making it harder to control access to data.

Backups:
DBMS ensures durability by providing mechanisms like transaction logs and backups, making it more robust against data loss due to hardware failures, crashes, or other unforeseen events.

Scalability:
DBMS is designed to scale both vertically (adding more resources to a single server) and horizontally (distributing data across multiple servers). This scalability is often more challenging to achieve with file storage.
Types of Databases:

Relational Databases (RDBMS):
Organize data into tables with rows and columns, establishing relationships between tables using keys.
Examples: MySQL, PostgreSQL, Oracle Database, Microsoft SQL Server, SQLite.

Non-Relational or NoSQL Databases:
Designed to handle unstructured or semi-structured data and offer more flexible data models. They don't rely on a fixed schema.
Examples: MongoDB, Cassandra, Redis, CouchDB, Neo4j.

Other Databases:

Document Stores:
Store and retrieve semi-structured data in document formats, commonly using JSON or BSON.
Examples: MongoDB, CouchDB, Elasticsearch.

Key-Value Stores:
Simplest form of NoSQL databases, storing data as key-value pairs, suitable for quick and straightforward data retrieval.
Examples: Redis, DynamoDB, Riak.

In-Memory Databases:
Store and retrieve data entirely in RAM for faster access and lower latency.
Examples: Redis (can also be considered an in-memory database), SAP HANA.

NewSQL Databases:
A class of modern relational databases that aim to provide the scalability of NoSQL databases while maintaining ACID properties.
Examples: Google Spanner, CockroachDB.

Graph Databases:
Designed for handling data with complex relationships and interconnectedness, often represented as nodes and edges.
Examples: Neo4j, ArangoDB, Amazon Neptune.

Column-Family Stores (Wide-Column Stores):
Optimize data storage for querying and retrieving specific columns rather than entire rows.
Examples: Apache Cassandra, HBase.


Which one is best?
The choice of a database type depends on the specific requirements of the application, the nature of the data, and the desired scalability and performance characteristics. Each type of database has its strengths and weaknesses, and the selection should align with the specific use case and goals of the project.



Difference B/W Relational Databses & Non-Relational Databases:

Relational databases (RDBMS) and non-relational databases (NoSQL) differ in their data models, schema flexibility, and use cases. Here are some key differences between the two:

1. Data Model:

Relational Database:
Organizes data into tables with predefined columns and data types.
Enforces a fixed schema, and data must conform to this structure.
Establishes relationships between tables using keys (primary and foreign keys).

Non-Relational Database (NoSQL):
Employs various data models, including document-oriented, key-value pairs, graph, column-family, etc.
Offers flexibility in data representation, allowing for dynamic and varying schemas.
Does not necessarily rely on the traditional table-based structure.

2. Schema:
Relational Database:
Enforces a rigid, predefined schema.
Changes to the schema can be complex and may require downtime for migration.

Non-Relational Database (NoSQL):
Offers dynamic schema, allowing for more flexibility.
New fields can be added to records without affecting existing records.

3. Scalability:

Relational Database:
Vertical scaling (adding more resources to a single server) is a common approach and is easy to accomplish, however horizontal scaling is complex to achieve.
May have limitations in handling massive amounts of data and high transaction rates.

Non-Relational Database (NoSQL):
Horizontal scaling (adding more servers to a distributed system) is often more straightforward.
Suited for handling large volumes of data and high levels of concurrent transactions.

4. Use Cases:
Relational Database:
Well-suited for applications with complex relationships and structured data.
Commonly used in traditional business applications, financial systems, and applications where ACID properties are crucial.

Non-Relational Database (NoSQL):
Suited for scenarios with dynamic or evolving schemas, unstructured or semi-structured data, and where horizontal scalability is important.
Commonly used in web applications, big data processing, real-time applications, and scenarios where flexible data models are needed.

5. Query Language:

Relational Database:
Typically uses SQL (Structured Query Language) for querying and manipulating data.
Supports complex queries involving multiple tables.

Non-Relational Database (NoSQL):
Query languages vary between different types of NoSQL databases.
Some NoSQL databases use specialized query languages tailored to their data models.

6. ACID Properties:

Relational Database:
Adheres to ACID properties (Atomicity, Consistency, Isolation, Durability) to ensure transactional integrity.

Non-Relational Database (NoSQL):
May relax some ACID properties to achieve better performance or scalability in certain scenarios. Some NoSQL databases opt for eventual consistency.

Which one is best?
In summary, the choice between a relational and a non-relational database depends on the specific requirements of the application, the nature of the data, and scalability needs. Relational databases are well-established and suitable for structured data with complex relationships, while non-relational databases offer more flexibility and scalability for scenarios with dynamic or evolving data structures.


##
Intension (Database Schema) Vs Extension (Database Instance):

#
Intension (Database Schema):
The intension of a database refers to the overall design or structure of the database. It encompasses the schema, which defines the logical organization of data, including tables, relationships, constraints, and other database objects.

Key Points:
The intension represents the static, unchanging aspects of the database.
It defines the data model, specifying the types of data that can be stored, relationships between entities, and constraints on the data.
Changes to the intension typically involve alterations to the database schema, such as adding or modifying tables or relationships.

Extension (Database Instance):
The extension of a database refers to the actual data stored in the database at a specific point in time. It represents the dynamic, changing aspect of the database as data is inserted, updated, or deleted.

Key Points:
The extension is the collection of all the rows and records currently present in the database tables.
It evolves as data is manipulated through operations like insert, update, and delete.
The extension is also known as the database instance, reflecting the current state of the database.


##
Centralized Databases vs Decentralized Databases:
Centralized databases and distributed databases differ in their architecture and how they handle data storage and processing. Here are the key differences between the two:

1. Definition:

Centralized Database:
A single, centralized database server stores and manages all the data.
Access to the database is usually through a single point, and all processing occurs on this central server.

Distributed Database:
Data is distributed across multiple nodes or servers, each with its own database instance.
Processing and data storage are distributed, and nodes can work together to perform tasks.

2. Architecture:

Centralized Database:
Has a single point of control and management.
Typically follows a client-server architecture, where clients interact with a central database server.

Distributed Database:
Distributed architecture with multiple nodes interconnected.
Can be structured in various ways, such as client-server, peer-to-peer, or a combination of both.

3. Data Distribution:

Centralized Database:
All data is stored in a single location.
Limited by the capacity and resources of the central server.

Distributed Database:
Data is distributed across multiple nodes, allowing for better scalability and performance.
Enables horizontal scaling by adding more nodes to the distributed system.

4. Performance:

Centralized Database:
Performance is dependent on the capacity and resources of the central server.
Scaling may be limited, and bottlenecks can occur.

Distributed Database:
Can provide better performance and scalability by distributing the workload among multiple nodes.
Handling of parallel processing and increased capacity can enhance performance.

5. Fault Tolerance:

Centralized Database:
Vulnerable to a single point of failure. If the central server fails, the entire system may be affected.

Distributed Database:
Offers better fault tolerance. If one node fails, other nodes can still operate, maintaining system functionality.

6. Data Consistency:

Centralized Database:
Generally simpler to maintain data consistency due to a single point of control.

Distributed Database:
Ensuring data consistency across distributed nodes can be more complex, requiring mechanisms like distributed transactions or eventual consistency.


7. Network Dependency:

Centralized Database:
Less dependent on network communication for data access since all data is stored centrally.

Distributed Database:
Relies on network communication for data access and coordination between nodes.

8. Use Cases:

Centralized Database:
Suitable for smaller-scale applications or situations where a single server can meet performance and capacity requirements.

Distributed Database:
Ideal for large-scale applications, big data processing, and scenarios where scalability, fault tolerance, and parallel processing are essential.

##
OLTP (Online Transaction Processing) 
VS
OLAP (Online Analytical Processing):


# OLTP (Online Transaction Processing):

OLTP, or Online Transaction Processing, is a class of database systems that manage and process transactions in real-time. These transactions are typically short, simple, and involve the day-to-day operations of an organization. OLTP systems are designed to handle a high volume of transactions and ensure data integrity. Characteristics of OLTP systems include:

Purpose:
OLTP is designed for transactional processing, focusing on routine and operational tasks such as data insertions, updates, and deletions.

Normaliztion:
Normalized data structures to minimize redundancy and ensure consistency in transactional operations.

Database Size:
Relatively smaller database size, typically dealing with current and recent data required for daily operations.

Workload:
Involves a high volume of transactions with low query complexity, emphasizing quick response times.

Response Time:
Low response time is a priority to efficiently process individual transactions.

Concurrency Control:
Emphasizes concurrency control mechanisms to manage multiple users concurrently accessing and modifying the data.

Example Use Cases:
Online banking systems, e-commerce platforms, reservation systems.


# OLAP (Online Analytical Processing):

OLAP, or Online Analytical Processing, is a category of database systems designed for complex data analysis and reporting. OLAP systems are optimized for queries that involve aggregations, calculations, and historical data. They support decision-making processes by providing insights into trends, patterns, and relationships within the data. Characteristics of OLAP systems include:

Purpose:
OLAP is designed for analytical processing, supporting complex queries for decision support and business intelligence.

DeNormaliztion:
Denormalized or partially denormalized data structures to facilitate complex queries and aggregations.

Database Size:
Involves larger database sizes, often storing historical data and aggregated information for analysis.

Workload:
Involves low transaction volume but high query complexity, handling complex analytical queries.
Response Time:
Response time can vary, and longer response times are tolerated for complex analytical queries.

Concurrency Control:
Generally read-intensive with limited concurrent updates, as OLAP processes focus more on analysis than frequent updates.

Example Use Cases:
Business intelligence systems, data warehouses, decision support systems.


In summary, OLTP systems are optimized for quick and efficient transactional processing, while OLAP systems are designed for in-depth data analysis and reporting, supporting decision-making processes within an organization. Both OLTP and OLAP systems play crucial roles in an enterprise's data architecture, addressing different aspects of data management and utilization.


###
Data Warehouse VS Data Lake VS Data Lakehouse

# Data Warehouse:
A data warehouse is a centralized repository that stores structured and organized data from various sources. It is designed for efficient querying and reporting, providing a consolidated view of historical and current data to support business intelligence and decision-making.

Characteristics:
Optimized for analytics and reporting.
Uses a structured, relational database model.
Supports Extract, Transform, Load (ETL) processes to transform and load data into the warehouse.
Enforces data quality and consistency.

Use Cases:
Business intelligence, data analysis, reporting.

# Data Lake:
A data lake is a storage repository that can hold vast amounts of raw, unstructured, or structured data. It allows for the storage of data in its raw format, preserving its original form. Data lakes enable organizations to store diverse data types at scale, facilitating data exploration, analytics, and machine learning.

Characteristics:
Stores raw, unprocessed data.
Supports a variety of data types, including structured, semi-structured, and unstructured.
Flexible schema or schema-on-read approach.
Can accommodate large volumes of data from different sources.

Use Cases:
Big data analytics, data exploration, machine learning, storing raw data for future use.

# Data Lakehouse:
A data lakehouse is an emerging concept that combines the features of both a data warehouse and a data lake. It seeks to integrate the benefits of structured processing and data governance from data warehouses with the scalability and flexibility of data lakes. The term is often associated with platforms that allow organizations to manage and process structured and unstructured data in a unified environment.

Characteristics:
Integrates structured and unstructured data processing.
Combines elements of a data warehouse and a data lake.
Aims to provide the best of both worlds in terms of analytics, governance, and scalability.
Enables organizations to work with both traditional structured data and newer, diverse data types.

Use Cases:
Comprehensive data processing, analytics, and reporting that leverage both structured and unstructured data.

# Summay
In summary, a data warehouse is designed for structured data analytics and reporting, a data lake is a storage repository for raw and diverse data types, and a data lakehouse combines features of both to provide a unified platform for processing and analyzing structured and unstructured data. The choice between these concepts depends on the specific needs and goals of an organization's data strategy.



##
Data abstraction:
Data abstraction in the context of databases refers to the process of hiding the complex implementation details of the database system and presenting users with a simplified and abstracted view of the data. It involves creating a conceptual model that allows users to interact with the database without needing to understand the underlying complexities of how the data is stored, organized, or processed.

# Levels
There are three main levels of data abstraction in a database system:

Physical Level (Internal View):
The lowest level of abstraction, dealing with how data is physically stored on the hard disk, including details such as storage mechanisms, file organization, and indexing.
Concerned with the low-level implementation details that are relevant to database administrators and developers involved in system optimization.

Logical Level (Conceptual View):
Represents the logical organization of data in the database, providing an abstraction of the actual data and its relationships.
Focuses on defining entities, attributes, relationships, and constraints without specifying how the data is physically stored.
Presents a conceptual model that is more closely aligned with the way users perceive the data.

View Level (External View or User View):
Represents the user-specific view of the data and provides a higher level of abstraction.
Defines how a particular user or group of users sees and interacts with the data, including specific subsets of information and customized queries.
Enables different users to have different views of the same underlying data, tailored to their needs.

# Advantages of Data Abstraction:

Simplifies Complexity: 
Users can interact with the database using a simplified and user-friendly conceptual model without being burdened by the intricacies of the database's internal workings.

Enhances Security: 
Access to data is controlled through the view level, allowing administrators to define specific user permissions and restrict access to sensitive information.

Promotes Modularity: 
Changes to the physical or logical level do not necessarily impact the view level, promoting modularity and ease of maintenance.

Facilitates Database Evolution: 
The abstraction layers provide flexibility for making changes to the database system without disrupting the applications or users interacting with the data.

Supports Multiple Views: 
Different users or applications can have their own customized views of the data, tailored to their specific needs.



##
Two-Tier VS Three-Tier Architecture:

# Two-Tier Architecture:
In a two-tier architecture, also known as a client-server architecture, the application logic is divided into two components: the client and the server.

Client Tier:
The client is responsible for the presentation layer and user interface.
It interacts directly with the user, handling user input and presenting information.
The client communicates with the server to request data or perform actions.

Server Tier:
The server is responsible for the application logic and data processing.
It receives requests from the client, processes them, and interacts with the database if needed.
The server sends the results back to the client for presentation.

Advantages of Two-Tier Architecture:
Simple and easy to manage.
Direct communication between the client and the database server.
Suitable for small-scale applications with a limited number of users.

Disadvantages of Two-Tier Architecture:
Lack of scalability for larger applications.
Maintenance and updates require changes on both the client and server.
Limited separation of concerns, making it less modular.

# Three-Tier Architecture:
In a three-tier architecture, the application logic is divided into three components: the presentation tier, the application (or business logic) tier, and the data (or database) tier.

Presentation Tier (Client) OR Frontend Tier:
Responsible for the user interface and user interaction.
Communicates with the application server to request and display data.

Application Tier (Server) Or API Tier:
Manages the application logic and business processes.
Processes requests from the presentation tier and interacts with the data tier.

Data Tier (Database):
Stores and manages the data.
Handles data storage, retrieval, and manipulation.
Isolated from the application logic, providing a centralized data repository.

Advantages of Three-Tier Architecture:
Improved modularity and separation of concerns.
Easier maintenance and updates, as changes in one tier do not necessarily affect the others.
Better scalability for larger and more complex applications.

Disadvantages of Three-Tier Architecture:
Increased complexity compared to two-tier architecture.
Overhead introduced by the additional layer (application server).
Requires network communication between tiers, which can introduce latency.

# Which one is best?
In summary, the choice between two-tier and three-tier architectures depends on the specific requirements and scale of the application. Smaller applications with limited complexity may benefit from the simplicity of a two-tier architecture, while larger and more complex applications often adopt a three-tier architecture for improved scalability and maintainability.


##
Data model:
A data model is a conceptual representation of how data is structured and organized within a database. It defines the relationships between different data elements and the rules governing those relationships. Data models serve as blueprints for designing and implementing databases, providing a clear and abstract view of the data and its structure. There are several types of data models, and each serves a specific purpose in the database design process. The main types include:

Relational Data Model:
Represents data as tables (relations) with rows and columns.
Defines relationships between tables using keys.
Widely used in modern database management systems (DBMS) such as MySQL, PostgreSQL, and Oracle.

Entity-Relationship (ER) Model:
Represents entities (objects) and their relationships in a graphical manner.
Entities have attributes, and relationships describe how entities are connected.
Commonly used during the initial stages of database design.

Hierarchical Data Model:
Represents data in a tree-like structure with a single root and multiple levels.
Each parent node can have multiple child nodes.
Commonly used in older database systems.

Network Data Model:
Extends the hierarchical model by allowing nodes to have multiple parents (owners).
Uses set theory to represent complex relationships.
Provides more flexibility compared to the hierarchical model.

Object-Oriented Data Model:
Extends the concepts of object-oriented programming to databases.
Represents data as objects with attributes and methods.
Suitable for modeling complex and interconnected data.

Object-Relational Data Model:
Integrates object-oriented features into the relational model.
Supports complex data types, inheritance, and encapsulation.
Offers advantages for handling multimedia and complex data.

NoSQL Data Models:

Various data models, including document, key-value, column-family, and graph models.
Used in NoSQL databases like MongoDB, Redis, Cassandra, and Neo4j.
Designed to handle specific data storage and retrieval requirements.

# Key Components of a Data Model:
Entities: Represent real-world objects or concepts.
Attributes: Describe properties or characteristics of entities.
Relationships: Define connections and associations between entities.
Constraints: Specify rules and limitations on data to maintain integrity.

# Purpose of Data Models:
Facilitate Communication: 
Provides a common language for communication between stakeholders, including designers, developers, and users.
Guide Database Design: 
Serves as a blueprint for structuring databases, guiding the creation of tables, relationships, and constraints.
Ensure Data Integrity: 
Enforces rules and constraints to maintain the accuracy and consistency of data.
Support Documentation: 
Documents the structure and relationships within a database, aiding in understanding and maintenance.



##
Steps in Database Design or Data Modelling:
Database design is a systematic process that involves several steps, from gathering requirements to implementing and maintaining a functional database system. The four major steps in database design are:

Requirements Analysis:
Objective: 
Understand and gather the requirements of the system or application.

Activities:
Conduct interviews with stakeholders.
Identify data entities, relationships, and attributes.
Define business rules and constraints.
Outcome: A clear understanding of what the database is supposed to achieve and the data it needs to store.

# Conceptual Design:
Objective: 
Create a high-level conceptual model that represents the data and its relationships.

Activities:
Use techniques like Entity-Relationship (ER) modeling.
Identify major entities and their relationships.
Develop an ER diagram to illustrate the conceptual model.

Outcome:
A visual representation of entities, attributes, and relationships, serving as a conceptual blueprint for the database.

# Logical Design:
Objective: 
Translate the conceptual model into a logical model that can be implemented in a specific database management system (DBMS).

Activities:
Choose a data model (e.g., relational model).
Specify data types, constraints, and primary keys.
Create a logical schema for tables, views, and indexes.

Outcome: 
A detailed logical model that defines the structure of the database, including tables, fields, relationships, and constraints.

# Physical Design:
Objective: 
Define how the logical model will be implemented in a specific DBMS, considering storage structures and optimization.

Activities:
Specify storage structures (tables, indexes, partitions).
Define primary and foreign key constraints.
Optimize for performance considerations.

Outcome: 
A detailed physical model that addresses the specifics of data storage, indexing, and performance optimization for the chosen DBMS.


##
ERD:

##
Relational Modal:
The relational model is a database model that represents data as tables with rows and columns, where each table is a relation, and each row in the table is a tuple (record) containing attributes (fields or columns). This model was introduced by Edgar F. Codd in 1970, and it has become the dominant data model for database management systems (DBMS).

Customers Table:

+--------------------+-----------------------------+----------------------+
| CustomerID |   CustomerName   |  City		  |
+-------------------+------------------------------+-----------------------+
|    101    	|   John Doe	   	|  NYC   	   |
|    102    	|   Jane Smith  	|  LA    		   |
|    103    	|   Bob Johnson 	|  Chicago 	   |
+--------------------+--------------+--------------+----------------------+


##
Entity:
An entity is a real-world object or concept that has an existence and can be uniquely identified. For example, in a university database, entities could include students, courses, and professors.
Represented as a rectangle in an ERD, with the entity's name inside the rectangle.
    
Entity Type:
An entity type is a collection of entities that share common characteristics or properties. It is a classification or category of entities. For example, "Student" and "Professor" are entity types.

Entity Set:
An entity set is a collection of entities of the same entity type. It represents all instances of a particular entity type at a specific point in time. For instance, the set of all students currently enrolled in a university is an entity set.

Strong Entity:
A strong entity is an entity that has a primary key attribute, and its existence is independent of other entities. It can be uniquely identified by its own attributes.
Represented as a single rectangle. 

Weak Entity:
A weak entity is an entity that does not have a primary key attribute of its own. Its existence is dependent on the existence of another entity, known as the "owning" or "parent" entity. A weak entity is identified by a partial key, which includes a partial set of attributes and relies on the owning entity for identification.
Represented as a double rectangle. A double diamond represents the identifying relationship with the owner entity.

Attributes:
Attributes are properties or characteristics that describe aspects of an entity or relationship. For a "Student" entity, attributes could include "StudentID," "Name," and "DateOfBirth."
Represented as ovals connected to the entity rectangle. Attributes are listed inside these ovals.


Domain:
A domain is the set of all possible values that an attribute can take. For example, the domain of the "DateOfBirth" attribute might be all valid dates.


# Types of Attributes:

Single-valued Attribute:
An attribute that holds a single value for each entity.
Represented by an oval connected to the entity rectangle.

Multivalued Attribute:
An attribute that can hold multiple values for each entity.
Represented by a double oval connected to the entity rectangle.

Simple Attribute:
An attribute that cannot be divided into sub-parts.
Represented by a single oval or double oval (for multivalued attributes) connected to the entity rectangle.

Composite Attribute:
An attribute that can be divided into sub-parts, each with its own meaning.
Represented by an oval with subparts connected by lines to the main oval. The subparts may be single or multivalued attributes.

Derived Attribute:
An attribute whose value is derived or calculated based on other attributes.
Represented by a dashed oval connected to the entity rectangle. The derivation method may be indicated.

Prime Attribute:
An attribute that is part of the primary key.
The primary key attributes are usually underlined in ERD.

Non-Prime Attribute:
An attribute that is not part of the primary key.
Non-prime attributes are not underlined in ERD.

Key:
A key is an attribute or combination of attributes that uniquely identifies an entity within an entity set. A key is crucial for distinguishing one entity from another. In a "Student" entity set, the "StudentID" might be a key.
Primary Key:
A primary key is a unique identifier for a record in a table. It uniquely identifies each row, and no two rows can have the same primary key value.

Rules:
Primary key values cannot be NULL.
Each table can have only one primary key.
Purpose: Uniquely identifies each record in a table.

Foreign Key:
A foreign key is a column or set of columns in one table that refers to the primary key in another table. It establishes a relationship between the two tables.

Rules:
Values in the foreign key must match values in the primary key of the referenced table.
Foreign key values can be NULL if the relationship is optional.
Purpose: Maintains referential integrity between related tables.

Candidate Key:
A candidate key is a set of one or more columns that can uniquely identify a record in a table. It is a potential candidate for being the primary key.

Rules:
Candidate keys must be unique.
They must not contain NULL values.
Purpose: Provides alternative choices for the primary key.

Super Key:
A super key is a set of one or more columns that, taken together, can uniquely identify a record in a table. It may include more columns than necessary to form a minimal candidate key.
Example: If a table has columns A, B, and C, then {A, B} and {A, B, C} are both super keys.
Purpose: Represents all possible combinations of columns that uniquely identify records.


##
Constraints:
Domain Constraint:
A domain constraint defines the allowable values for an attribute. It specifies the range of valid data values that a particular attribute can take.
Example: If an attribute "Age" has a domain constraint specifying that it must be a positive integer, any attempt to insert a negative number or a non-integer value would violate the domain constraint.
Purpose: Ensures that data adheres to predefined rules for valid values in specific attributes.

Key Constraint or Unique Constraint:
A key constraint ensures the uniqueness of values within a specified column or combination of columns. It can be applied to a primary key or a unique key.
Example: If a table has a unique constraint on the "Email" column, each email address must be unique within that column.
Purpose: Prevents duplication of values in specified columns, maintaining data integrity.

Entity Integrity Constraint:
Entity integrity ensures that each row (tuple) in a table is uniquely identified by a primary key, and also null value cannot be acceptable. It's a specific case of the key constraint applied to the primary key column(s).
Example: In a "Customers" table, the entity integrity constraint ensures that each customer is uniquely identified by the primary key "CustomerID."
Purpose: Guarantees the uniqueness and non-null nature of primary key values for each entity in a table.

Referential Entity Integrity Constraint:
This constraint is a specific type of referential integrity constraint that ensures the consistency of relationships between tables. It requires that foreign key values in one table correspond to primary key values in another table.
Example: If a "Orders" table has a foreign key "CustomerID" referring to the "Customers" table, the referential entity integrity constraint ensures that every "CustomerID" in the "Orders" table corresponds to a valid "CustomerID" in the "Customers" table.
Purpose: Maintains the integrity of relationships between tables by ensuring that foreign key values reference valid primary key values in related tables.


##
Relationship:
A relationship is a meaningful association or link between two or more entities. It represents a connection that exists when entities are related in some way.
In an ERD, a relationship is typically represented by a diamond shape connecting two or more entities. The diamond contains a label that describes the nature of the relationship.

Example:
Consider entities "Student" and "Course." A relationship between them could be "Enroll," representing the association between students and the courses they are enrolled in. In this example, the "Enroll" relationship connects the "Student" and "Course" entities.

Unary Relationship:
A unary relationship involves a single entity type. In other words, it's a relationship where an entity is related to itself.
Example: Consider an "Employee" entity with a unary relationship "Manages" to represent the relationship where an employee manages other employees. In this case, the "Manages" relationship is unary because it connects instances of the same entity type (Employee).

Binary Relationship:
A binary relationship involves two distinct entity types. It is the most common type of relationship in relational database design.
Example: In a university database, a binary relationship "Enroll" might exist between the "Student" and "Course" entities, indicating that students enroll in courses. The "Enroll" relationship connects instances of the "Student" entity to instances of the "Course" entity.

Ternary Relationship:
A ternary relationship involves three distinct entity types. It represents a more complex association where instances of three entities are related to each other.
Example: In a hospital database, a ternary relationship "Admission" might exist between the "Patient," "Doctor," and "Room" entities, indicating the admission of a patient to a specific room under the care of a particular doctor.



##
Degree OR Degree of Relationship:
The degree of a relationship in a database refers to the number of entities participating in a relationship.
Example: In a binary relationship between two entities A and B, the degree is 2 (binary).

Degree of Cardinality:
Refers to the number of entities or occurrences involved in a cardinality specification.
Example: In a "one-to-many" relationship, the degree of cardinality is 2, as it involves two entities (one from each side).

Cardinality or Cardinality Ratio or Cardinality Mapping:
Cardinality defines the maximum number of occurrences of one entity that can be associated with the maximum number of occurrences of another entity in a relationship.
Example: In a "one-to-many" relationship, the cardinality from the "one" side is 1, and from the "many" side is many.


Cardinality Ratio:
Describes the proportion of entities participating in a relationship based on the specified cardinality.
Example: In a "one-to-many" relationship, the cardinality ratio is 1:N, indicating one occurrence on one side can be associated with many occurrences on the other side.


One-to-One (1:1):
Each instance on one side of the relationship is related to only one instance on the other side, and vice versa.
Representation: "One-to-One" or "1:1."
Example: One student has one roll no. Or One Citizen has on CNIC

One-to-Many (1:N):
Each instance on one side of the relationship is related to multiple instances on the other side, but each instance on the other side is related to only one instance on the first side.
Representation: "One-to-Many" or "1:N."

Many-to-One (M:1):
Multiple instances on one side of the relationship are related to a single instance on the other side.
Representation: "Many-to-One" or "M:1." (Reverse of One-to-Many)

Many-to-Many (M:N):
Multiple instances on one side of the relationship can be related to multiple instances on the other side.
Representation: "Many-to-Many" or "M:N."



# Participation or Existence:
In the context of Entity-Relationship Diagrams (ERDs) and database design, participation or existence refers to the degree to which entities in a relationship are involved or required. 

There are two main types of participation: total participation and partial participation.

Total Participation:
Total participation (also known as existence dependency) occurs when every entity in an entity set must participate in the relationship. In other words, an entity on one side of the relationship must be associated with at least one entity on the other side.
Representation:
In an ERD, total participation is typically indicated by double lines connecting the participating entity to the relationship line.

Partial Participation:
Partial participation occurs when some entities in an entity set may choose not to participate in the relationship. It allows for entities to be independent of the relationship and not necessarily associated with entities on the other side.
Representation: In an ERD, partial participation is indicated by a single line connecting the participating entity to the relationship line.


##
Extended Entity-Relationship (ER) features:
Extended Entity-Relationship (ER) features, also known as Enhanced Entity-Relationship features, are additions to the traditional ER model designed to provide more expressive power in capturing complex relationships and constraints in a database. These extensions help in modeling more sophisticated database scenarios. Some of the commonly used extended ER features include:

Subtypes and Supertypes (Inheritance):
Allows entities to inherit attributes from a higher-level entity, creating a subtype-supertype relationship.
Use Case: Useful when entities share common attributes but also have unique attributes.
Representation: Represented by a triangle connecting the subtype entities to the supertype entity.

Specialization and Generalization:
Similar to subtypes and supertypes, specialization and generalization allow the modeling of hierarchies within entities.
Use Case: Useful when entities have common attributes but may also have specialized attributes based on their roles or characteristics.
Representation: Similar to subtypes and supertypes, represented by a triangle connecting the specialized entities to the generalized entity.

Union Types:
Enables an entity to be a member of more than one entity type.
Use Case: Useful when an entity exhibits different characteristics or attributes based on its role in various contexts.
Representation: Represented by combining different entity types into a single entity with attributes from all participating entity types.

Aggregation:
Represents a relationship between an entity and a relationship set.
Use Case: Useful when a relationship between entities is itself an entity with additional attributes.
Representation: Represented by a diamond shape connecting an entity to a relationship set.

Attributes Inheritance:
Allows attributes to be inherited by entities in subtypes.
Use Case: Useful when certain attributes are common to all entities in a subtype.
Representation: Attributes are specified in the supertype, and subtypes inherit these attributes.

Constraint Disjointness:
Specifies that an entity can only be a member of one subtype within a set of subtypes.
Use Case: Useful when ensuring that an entity belongs to only one subtype within a hierarchy.
Representation: Indicated by "disjoint" notation in the subtype-supertype relationship.

Constraint Completeness:
Specifies whether an entity must belong to at least one subtype within a set of subtypes.
Use Case: Useful when ensuring that an entity participates in the specialization hierarchy.
Representation: Indicated by "complete" or "incomplete" notation in the subtype-supertype relationship.



##
Normalization:
Normalization is the process of organizing and structuring data in a relational database to reduce redundancy, eliminate data anomalies, and improve data integrity. The goal of normalization is to ensure that data is stored efficiently without unnecessary duplication and to maintain consistency and accuracy across the database.

The normalization process involves breaking down large tables into smaller, related tables and organizing the data in a way that adheres to specific rules or normal forms. 

Stages:
First Normal Form (1NF):
Second Normal Form (2NF):
Third Normal Form (3NF):
 

Problems without Normaliztion /Anomalies (Insert, Update, Delete):
Without normalization, a database may suffer from several problems, often referred to as anomalies, which can impact data integrity, consistency, and efficiency. Here are three common types of anomalies that can occur without normalization:

Insertion Anomalies:
Insertion anomalies occur when adding new data to the database leads to difficulties or inconsistencies.
Example: In a denormalized table where customer information and order details are stored together, attempting to insert a new order for a customer who hasn't made any previous orders results in incomplete or redundant information.

Update Anomalies:
Update anomalies arise when updating data in the database leads to inconsistencies or requires modifications in multiple places.
Example: In a denormalized table where customer addresses are stored with each order, if a customer moves and their address needs to be updated, it must be updated in multiple rows (orders) leading to potential errors or inconsistencies.

Deletion Anomalies:
Deletion anomalies occur when removing data from the database results in unintended loss of related information.
Example: In a denormalized table where customer information is repeated for each order, deleting a single order might inadvertently remove the only record of a customer, leading to the loss of customer details.

Normalization addresses these anomalies by breaking down tables into smaller, related tables and organizing the data according to specific rules (normal forms). 

# Why We Need Normalization:
Minimize Redundancy & Anamolies: By storing data in a structured and non-redundant way, normalization reduces the need for duplicate information.

Improve Consistency: Normalization ensures that data dependencies are well-defined, reducing the likelihood of inconsistencies and anomalies.

Facilitate Maintenance: Normalized databases are typically easier to maintain and modify, as changes are localized to specific tables.

While normalization provides these benefits, it's essential to strike a balance and consider the specific requirements of the application. Over-normalization may lead to more complex query structures and could impact performance, so the level of normalization should be chosen based on the specific needs of the database system.

#
Terms Used in Normalization:

# Functional dependency:
Functional dependency defines how the value of one attribute uniquely determines the value of another attribute. In other words, if the value of one attribute (or set of attributes) uniquely determines the value of another attribute, a functional dependency exists.


The notation for a functional dependency is represented as X → Y, where X and Y are sets of attributes, and the arrow (→) signifies that the values of X uniquely determine the values of Y.

If X → Y holds, it means that for any two tuples (rows) in the relation with the same values for X, the corresponding values for Y must also be the same.
Here are a few examples to illustrate functional dependencies:

A functional Dependency A→B always holds if A is key in the relation R and A,B are the attributes or set of attributes.

Product Table:
In a "Product" table with attributes: {ProductID, ProductName, Price}.
If ProductID uniquely determines ProductName, you can express it as ProductID → ProductName.
If ProductID uniquely determines Price, you can express it as ProductID → Price.
Functional dependencies play a crucial role in the normalization process. The process of normalization involves identifying and eliminating redundant data by ensuring that the database schema satisfies certain normal forms based on functional dependencies. 

# Types of Functional Dependency:
Trivial Functional Dependencies & Non-Trivial Functional Dependencies:


Trivial Functional Dependencies:
A functional dependency X → Y is said to be trivial if and only if Y ⊆ X. Thus, if RHS of a functional dependency is a subset of LHS, then it is called a trivial functional dependency.

Non-Trivial Functional Dependencies:
A functional dependency X → Y is said to be non-trivial if and only if Y ⊄ X.
Thus, if there exists at least one attribute in the RHS of a functional dependency
that is not a part of LHS, then it is called a non-trivial functional dependency.


# Closure of an Attribute:
Closure of an attribute refers to the complete set of attributes that are functionally determined by the given attribute and function dependencies.

The closure of an attribute set is denoted by the symbol '+'. If we have a set of attributes A, the closure of A, denoted as A+, represents all the attributes that are functionally dependent on A.


Finding Closure:
The closure is calculated based on a set of functional dependencies. Here's how the closure operation is typically defined:

Closure of a Single Attribute:
If X is a single attribute, X+ includes X and all attributes that are functionally dependent on X.

Closure of a Set of Attributes (X):
If X is a set of attributes, X+ includes X and all attributes that can be functionally determined by the attributes in set X, according to the given set of functional dependencies.
The process of computing the closure involves repeatedly applying the given functional dependencies until no new attributes can be added to the closure.

Example:
Consider a set of functional dependencies:

A → B
B → C
C → D

If we want to find the closure of the attribute set {A}, we perform the following calculations:

A+ = {A} (initially)
A+ = {A, B} (applying A → B)
A+ = {A, B, C} (applying B → C)
A+ = {A, B, C, D} (applying C → D)

Therefore, the closure of {A} under the given set of functional dependencies is {A, B, C, D}.

Closure operations are essential for determining the attributes that are functionally dependent on a given set of attributes, which is crucial in the normalization process of a relational database.




#
Stages:

First Normal Form (1NF):
Requirements:
There should be no single column or attribute which contains multiple values. All the columns should contain single ( atomic ) values.

Example: 
If a table has a column or attribute with a list of multiple phone numbers, it is not in 1NF. To achieve 1NF, each phone number should be inside a separate table and linked via foreign key to the original table.

Second Normal Form (2NF):
Requirements:
Must be in 1NF.
There should be no any non-prime attribute which is partially dependent on the proper subset (part) of the key. 
OR
All non-prime attributes fully functionally dependent on key.

Partial dependency: Part of the key uniquely determines non-prime attributes.

In simple words, every table or relation should describe one entity, and every column in that table should describe that entity.

Example: 
If a table has a composite primary key (e.g., {StudentID, CourseID}) and non-prime attributes (e.g., Instructor) that depend on only one part of the key (e.g., CourseID), it is not in 2NF.

Third Normal Form (3NF):
Requirements:
Must be in 2NF.
There should be no any non-prime attribute in relation R or table which is transitively dependent on the key of relation or table.
OR
All the attributes in relation R or table is functionaly determined only by the candidate keys of that relation R or table, and not by any non-prime attributes.
OR 
A column in a table should not be derived other columns except candidate keys.

Example: If a table has a non-prime attribute (e.g., InstructorOffice) that depends on another non-prime attribute (e.g., Instructor), it is not in 3NF.


Boyce Codd Normal Form (BCNF):
A relation R is said to be in BCNF if it is already in 3NF and for every functional dependency A→B, A should be super key in R.




