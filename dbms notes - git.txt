Data:
Data refers to raw facts and figures. It can be in the form of numbers, text, images, or any other format. 
Data by itself is not meaningful, it becomes information when processed and organized.

Information:
Information is the result of processing and organizing data in a meaningful way. It provides context, relevance, and purpose, making it useful for decision-making.

Database:
A database is a structured collection of data that is organized (typically using tables with predefined columns and data types) and stored in a way that allows for efficient retrieval and manipulation.

Database Management System (DBMS):
A Database Management System is software that enables the creation, maintenance, and use of databases. It provides an interface for interacting with the database, ensuring data integrity, security, and efficient retrieval. Examples of DBMS include MySQL, Oracle, Microsoft SQL Server, and PostgreSQL.

Database System:
A Database System is a combination of a database and its DBMS along with hardware. It includes the database(stored data), DBMS(software that manages and provides access to that data.), and the hardware upon which this DBMS is installed or running.

Applications (Use-Cases) of DBMS:
Web Applications, Mobile Applications, E-commerce applications, Enterprise applications, Social networking Applications, Gaming Applications, Data science, Machine Learning etc



History of Data (File-Based Storage):
File-based storage or flat file storage, involves storing data in files on a file system. 

Why not use file-based? / Limitations:

Data Redundancy:
In file storage, data redundancy can be a common issue. If the same data needs to be stored in multiple files, any updates or changes to that data may lead to inconsistencies.

Limited Query Capabilities:
Retrieving specific pieces of data often involves reading through entire files, which can be inefficient and time-consuming, especially as the volume of data increases.

Data Isolation:
Each file in a file storage system is typically independent, making it challenging to establish relationships or connections between different sets of data. This can hinder the ability to perform retrieve related information.

Concurrency Issues:
File systems may not handle concurrent access well, leading to potential conflicts when multiple users or processes attempt to access or modify the same file simultaneously. This lack of concurrency control can result in data inconsistencies.

Limited Security Features:
File systems may lack robust security features compared to DBMS. Implementing access controls, authentication, and encryption can be more challenging in a file storage environment.

Scalability Challenges:
File storage systems might face scalability challenges as the volume of data increases. Managing a large number of files can become unwieldy, and optimizing performance may become difficult.

Data Integrity Concerns:
Maintaining data integrity in file storage systems can be challenging. There is a higher risk of data corruption or inconsistencies, especially if updates are not managed carefully.

Limited Support for Transactions:
File storage systems may not provide built-in support for transactions, making it harder to ensure atomicity, consistency, isolation, and durability (ACID properties) in data operations.

In summary, while file storage may be suitable for simple and small-scale applications, a DBMS offers a more robust, efficient, and secure solution for managing and accessing data, especially in environments with complex data relationships and a need for concurrent data access.


Solution: DBMS Advantages:

Transactions & ACID Properties: 
ACID (Atomicity, Consistency, Isolation, Durability) represents a set of properties ensuring the reliability and consistency of database transactions by guaranteeing that they are atomic, maintain data consistency, operate in isolation, and endure system failures.

Normalization:
DBMS minimizes data redundancy through normalization techniques, reducing the chances of inconsistencies and making updates or changes more manageable.

Data Integrity:
DBMS enforces data integrity through constraints (e.g., unique keys, foreign keys, and check constraints). This ensures that the data in the database is accurate and consistent.

Data Modeling & Relationships:
DBMS allows the use of data models and schema definitions, providing a clear structure for the data. DBMS also supports the establishment of relationships between different tables (entities) using keys. This relational structure allows for more efficient querying and retrieval of related data compared to flat file storage.

Concurrency Control:
DBMS provides mechanisms for handling multiple users accessing the database simultaneously while maintaining consistency. This is essential for applications with concurrent user activity.

Query Language:
DBMS uses a standardized query language like SQL, making it easier to retrieve, update, and manipulate data. This simplifies the process of interacting with the stored information.

Security:
DBMS offers robust security features, including user authentication, authorization, and encryption. File systems may lack these built-in security measures, making it harder to control access to data.

Backups:
DBMS ensures durability by providing mechanisms like transaction logs and backups, making it more robust against data loss due to hardware failures, crashes, or other unforeseen events.

Scalability:
DBMS is designed to scale both vertically (adding more resources to a single server) and horizontally (distributing data across multiple servers). This scalability is often more challenging to achieve with file storage.
Types of Databases:

Relational Databases (RDBMS):
Organize data into tables with rows and columns, establishing relationships between tables using keys.
Examples: MySQL, PostgreSQL, Oracle Database, Microsoft SQL Server, SQLite.

Non-Relational or NoSQL Databases:
Designed to handle unstructured or semi-structured data and offer more flexible data models. They don't rely on a fixed schema.
Examples: MongoDB, Cassandra, Redis, CouchDB, Neo4j.

Other Databases:

Document Stores:
Store and retrieve semi-structured data in document formats, commonly using JSON or BSON.
Examples: MongoDB, CouchDB, Elasticsearch.

Key-Value Stores:
Simplest form of NoSQL databases, storing data as key-value pairs, suitable for quick and straightforward data retrieval.
Examples: Redis, DynamoDB, Riak.

In-Memory Databases:
Store and retrieve data entirely in RAM for faster access and lower latency.
Examples: Redis (can also be considered an in-memory database), SAP HANA.

NewSQL Databases:
A class of modern relational databases that aim to provide the scalability of NoSQL databases while maintaining ACID properties.
Examples: Google Spanner, CockroachDB.

Graph Databases:
Designed for handling data with complex relationships and interconnectedness, often represented as nodes and edges.
Examples: Neo4j, ArangoDB, Amazon Neptune.

Column-Family Stores (Wide-Column Stores):
Optimize data storage for querying and retrieving specific columns rather than entire rows.
Examples: Apache Cassandra, HBase.


Which one is best?
The choice of a database type depends on the specific requirements of the application, the nature of the data, and the desired scalability and performance characteristics. Each type of database has its strengths and weaknesses, and the selection should align with the specific use case and goals of the project.



Difference B/W Relational Databses & Non-Relational Databases:

Relational databases (RDBMS) and non-relational databases (NoSQL) differ in their data models, schema flexibility, and use cases. Here are some key differences between the two:

1. Data Model:

Relational Database:
Organizes data into tables with predefined columns and data types.
Enforces a fixed schema, and data must conform to this structure.
Establishes relationships between tables using keys (primary and foreign keys).

Non-Relational Database (NoSQL):
Employs various data models, including document-oriented, key-value pairs, graph, column-family, etc.
Offers flexibility in data representation, allowing for dynamic and varying schemas.
Does not necessarily rely on the traditional table-based structure.

2. Schema:
Relational Database:
Enforces a rigid, predefined schema.
Changes to the schema can be complex and may require downtime for migration.

Non-Relational Database (NoSQL):
Offers dynamic schema, allowing for more flexibility.
New fields can be added to records without affecting existing records.

3. Scalability:

Relational Database:
Vertical scaling (adding more resources to a single server) is a common approach and is easy to accomplish, however horizontal scaling is complex to achieve.
May have limitations in handling massive amounts of data and high transaction rates.

Non-Relational Database (NoSQL):
Horizontal scaling (adding more servers to a distributed system) is often more straightforward.
Suited for handling large volumes of data and high levels of concurrent transactions.

4. Use Cases:
Relational Database:
Well-suited for applications with complex relationships and structured data.
Commonly used in traditional business applications, financial systems, and applications where ACID properties are crucial.

Non-Relational Database (NoSQL):
Suited for scenarios with dynamic or evolving schemas, unstructured or semi-structured data, and where horizontal scalability is important.
Commonly used in web applications, big data processing, real-time applications, and scenarios where flexible data models are needed.

5. Query Language:

Relational Database:
Typically uses SQL (Structured Query Language) for querying and manipulating data.
Supports complex queries involving multiple tables.

Non-Relational Database (NoSQL):
Query languages vary between different types of NoSQL databases.
Some NoSQL databases use specialized query languages tailored to their data models.

6. ACID Properties:

Relational Database:
Adheres to ACID properties (Atomicity, Consistency, Isolation, Durability) to ensure transactional integrity.

Non-Relational Database (NoSQL):
May relax some ACID properties to achieve better performance or scalability in certain scenarios. Some NoSQL databases opt for eventual consistency.

Which one is best?
In summary, the choice between a relational and a non-relational database depends on the specific requirements of the application, the nature of the data, and scalability needs. Relational databases are well-established and suitable for structured data with complex relationships, while non-relational databases offer more flexibility and scalability for scenarios with dynamic or evolving data structures.


##
Intension (Database Schema) Vs Extension (Database Instance):

#
Intension (Database Schema):
The intension of a database refers to the overall design or structure of the database. It encompasses the schema, which defines the logical organization of data, including tables, relationships, constraints, and other database objects.

Key Points:
The intension represents the static, unchanging aspects of the database.
It defines the data model, specifying the types of data that can be stored, relationships between entities, and constraints on the data.
Changes to the intension typically involve alterations to the database schema, such as adding or modifying tables or relationships.

Extension (Database Instance):
The extension of a database refers to the actual data stored in the database at a specific point in time. It represents the dynamic, changing aspect of the database as data is inserted, updated, or deleted.

Key Points:
The extension is the collection of all the rows and records currently present in the database tables.
It evolves as data is manipulated through operations like insert, update, and delete.
The extension is also known as the database instance, reflecting the current state of the database.


##
Centralized Databases vs Decentralized Databases:
Centralized databases and distributed databases differ in their architecture and how they handle data storage and processing. Here are the key differences between the two:

1. Definition:

Centralized Database:
A single, centralized database server stores and manages all the data.
Access to the database is usually through a single point, and all processing occurs on this central server.

Distributed Database:
Data is distributed across multiple nodes or servers, each with its own database instance.
Processing and data storage are distributed, and nodes can work together to perform tasks.

2. Architecture:

Centralized Database:
Has a single point of control and management.
Typically follows a client-server architecture, where clients interact with a central database server.

Distributed Database:
Distributed architecture with multiple nodes interconnected.
Can be structured in various ways, such as client-server, peer-to-peer, or a combination of both.

3. Data Distribution:

Centralized Database:
All data is stored in a single location.
Limited by the capacity and resources of the central server.

Distributed Database:
Data is distributed across multiple nodes, allowing for better scalability and performance.
Enables horizontal scaling by adding more nodes to the distributed system.

4. Performance:

Centralized Database:
Performance is dependent on the capacity and resources of the central server.
Scaling may be limited, and bottlenecks can occur.

Distributed Database:
Can provide better performance and scalability by distributing the workload among multiple nodes.
Handling of parallel processing and increased capacity can enhance performance.

5. Fault Tolerance:

Centralized Database:
Vulnerable to a single point of failure. If the central server fails, the entire system may be affected.

Distributed Database:
Offers better fault tolerance. If one node fails, other nodes can still operate, maintaining system functionality.

6. Data Consistency:

Centralized Database:
Generally simpler to maintain data consistency due to a single point of control.

Distributed Database:
Ensuring data consistency across distributed nodes can be more complex, requiring mechanisms like distributed transactions or eventual consistency.


7. Network Dependency:

Centralized Database:
Less dependent on network communication for data access since all data is stored centrally.

Distributed Database:
Relies on network communication for data access and coordination between nodes.

8. Use Cases:

Centralized Database:
Suitable for smaller-scale applications or situations where a single server can meet performance and capacity requirements.

Distributed Database:
Ideal for large-scale applications, big data processing, and scenarios where scalability, fault tolerance, and parallel processing are essential.

##
OLTP (Online Transaction Processing) 
VS
OLAP (Online Analytical Processing):


# OLTP (Online Transaction Processing):

OLTP, or Online Transaction Processing, is a class of database systems that manage and process transactions in real-time. These transactions are typically short, simple, and involve the day-to-day operations of an organization. OLTP systems are designed to handle a high volume of transactions and ensure data integrity. Characteristics of OLTP systems include:

Purpose:
OLTP is designed for transactional processing, focusing on routine and operational tasks such as data insertions, updates, and deletions.

Normaliztion:
Normalized data structures to minimize redundancy and ensure consistency in transactional operations.

Database Size:
Relatively smaller database size, typically dealing with current and recent data required for daily operations.

Workload:
Involves a high volume of transactions with low query complexity, emphasizing quick response times.

Response Time:
Low response time is a priority to efficiently process individual transactions.

Concurrency Control:
Emphasizes concurrency control mechanisms to manage multiple users concurrently accessing and modifying the data.

Example Use Cases:
Online banking systems, e-commerce platforms, reservation systems.


# OLAP (Online Analytical Processing):

OLAP, or Online Analytical Processing, is a category of database systems designed for complex data analysis and reporting. OLAP systems are optimized for queries that involve aggregations, calculations, and historical data. They support decision-making processes by providing insights into trends, patterns, and relationships within the data. Characteristics of OLAP systems include:

Purpose:
OLAP is designed for analytical processing, supporting complex queries for decision support and business intelligence.

DeNormaliztion:
Denormalized or partially denormalized data structures to facilitate complex queries and aggregations.

Database Size:
Involves larger database sizes, often storing historical data and aggregated information for analysis.

Workload:
Involves low transaction volume but high query complexity, handling complex analytical queries.
Response Time:
Response time can vary, and longer response times are tolerated for complex analytical queries.

Concurrency Control:
Generally read-intensive with limited concurrent updates, as OLAP processes focus more on analysis than frequent updates.

Example Use Cases:
Business intelligence systems, data warehouses, decision support systems.


In summary, OLTP systems are optimized for quick and efficient transactional processing, while OLAP systems are designed for in-depth data analysis and reporting, supporting decision-making processes within an organization. Both OLTP and OLAP systems play crucial roles in an enterprise's data architecture, addressing different aspects of data management and utilization.


###
Data Warehouse VS Data Lake VS Data Lakehouse

# Data Warehouse:
A data warehouse is a centralized repository that stores structured and organized data from various sources. It is designed for efficient querying and reporting, providing a consolidated view of historical and current data to support business intelligence and decision-making.

Characteristics:
Optimized for analytics and reporting.
Uses a structured, relational database model.
Supports Extract, Transform, Load (ETL) processes to transform and load data into the warehouse.
Enforces data quality and consistency.

Use Cases:
Business intelligence, data analysis, reporting.

# Data Lake:
A data lake is a storage repository that can hold vast amounts of raw, unstructured, or structured data. It allows for the storage of data in its raw format, preserving its original form. Data lakes enable organizations to store diverse data types at scale, facilitating data exploration, analytics, and machine learning.

Characteristics:
Stores raw, unprocessed data.
Supports a variety of data types, including structured, semi-structured, and unstructured.
Flexible schema or schema-on-read approach.
Can accommodate large volumes of data from different sources.

Use Cases:
Big data analytics, data exploration, machine learning, storing raw data for future use.

# Data Lakehouse:
A data lakehouse is an emerging concept that combines the features of both a data warehouse and a data lake. It seeks to integrate the benefits of structured processing and data governance from data warehouses with the scalability and flexibility of data lakes. The term is often associated with platforms that allow organizations to manage and process structured and unstructured data in a unified environment.

Characteristics:
Integrates structured and unstructured data processing.
Combines elements of a data warehouse and a data lake.
Aims to provide the best of both worlds in terms of analytics, governance, and scalability.
Enables organizations to work with both traditional structured data and newer, diverse data types.

Use Cases:
Comprehensive data processing, analytics, and reporting that leverage both structured and unstructured data.

# Summay
In summary, a data warehouse is designed for structured data analytics and reporting, a data lake is a storage repository for raw and diverse data types, and a data lakehouse combines features of both to provide a unified platform for processing and analyzing structured and unstructured data. The choice between these concepts depends on the specific needs and goals of an organization's data strategy.



##
Data abstraction:
Data abstraction in the context of databases refers to the process of hiding the complex implementation details of the database system and presenting users with a simplified and abstracted view of the data. It involves creating a conceptual model that allows users to interact with the database without needing to understand the underlying complexities of how the data is stored, organized, or processed.

# Levels
There are three main levels of data abstraction in a database system:

Physical Level (Internal View):
The lowest level of abstraction, dealing with how data is physically stored on the hard disk, including details such as storage mechanisms, file organization, and indexing.
Concerned with the low-level implementation details that are relevant to database administrators and developers involved in system optimization.

Logical Level (Conceptual View):
Represents the logical organization of data in the database, providing an abstraction of the actual data and its relationships.
Focuses on defining entities, attributes, relationships, and constraints without specifying how the data is physically stored.
Presents a conceptual model that is more closely aligned with the way users perceive the data.

View Level (External View or User View):
Represents the user-specific view of the data and provides a higher level of abstraction.
Defines how a particular user or group of users sees and interacts with the data, including specific subsets of information and customized queries.
Enables different users to have different views of the same underlying data, tailored to their needs.

# Advantages of Data Abstraction:

Simplifies Complexity: 
Users can interact with the database using a simplified and user-friendly conceptual model without being burdened by the intricacies of the database's internal workings.

Enhances Security: 
Access to data is controlled through the view level, allowing administrators to define specific user permissions and restrict access to sensitive information.

Promotes Modularity: 
Changes to the physical or logical level do not necessarily impact the view level, promoting modularity and ease of maintenance.

Facilitates Database Evolution: 
The abstraction layers provide flexibility for making changes to the database system without disrupting the applications or users interacting with the data.

Supports Multiple Views: 
Different users or applications can have their own customized views of the data, tailored to their specific needs.



##
Two-Tier VS Three-Tier Architecture:

# Two-Tier Architecture:
In a two-tier architecture, also known as a client-server architecture, the application logic is divided into two components: the client and the server.

Client Tier:
The client is responsible for the presentation layer and user interface.
It interacts directly with the user, handling user input and presenting information.
The client communicates with the server to request data or perform actions.

Server Tier:
The server is responsible for the application logic and data processing.
It receives requests from the client, processes them, and interacts with the database if needed.
The server sends the results back to the client for presentation.

Advantages of Two-Tier Architecture:
Simple and easy to manage.
Direct communication between the client and the database server.
Suitable for small-scale applications with a limited number of users.

Disadvantages of Two-Tier Architecture:
Lack of scalability for larger applications.
Maintenance and updates require changes on both the client and server.
Limited separation of concerns, making it less modular.

# Three-Tier Architecture:
In a three-tier architecture, the application logic is divided into three components: the presentation tier, the application (or business logic) tier, and the data (or database) tier.

Presentation Tier (Client) OR Frontend Tier:
Responsible for the user interface and user interaction.
Communicates with the application server to request and display data.

Application Tier (Server) Or API Tier:
Manages the application logic and business processes.
Processes requests from the presentation tier and interacts with the data tier.

Data Tier (Database):
Stores and manages the data.
Handles data storage, retrieval, and manipulation.
Isolated from the application logic, providing a centralized data repository.

Advantages of Three-Tier Architecture:
Improved modularity and separation of concerns.
Easier maintenance and updates, as changes in one tier do not necessarily affect the others.
Better scalability for larger and more complex applications.

Disadvantages of Three-Tier Architecture:
Increased complexity compared to two-tier architecture.
Overhead introduced by the additional layer (application server).
Requires network communication between tiers, which can introduce latency.

# Which one is best?
In summary, the choice between two-tier and three-tier architectures depends on the specific requirements and scale of the application. Smaller applications with limited complexity may benefit from the simplicity of a two-tier architecture, while larger and more complex applications often adopt a three-tier architecture for improved scalability and maintainability.


##
Data model:
A data model is a conceptual representation of how data is structured and organized within a database. It defines the relationships between different data elements and the rules governing those relationships. Data models serve as blueprints for designing and implementing databases, providing a clear and abstract view of the data and its structure. There are several types of data models, and each serves a specific purpose in the database design process. The main types include:

Relational Data Model:
Represents data as tables (relations) with rows and columns.
Defines relationships between tables using keys.
Widely used in modern database management systems (DBMS) such as MySQL, PostgreSQL, and Oracle.

Entity-Relationship (ER) Model:
Represents entities (objects) and their relationships in a graphical manner.
Entities have attributes, and relationships describe how entities are connected.
Commonly used during the initial stages of database design.

Hierarchical Data Model:
Represents data in a tree-like structure with a single root and multiple levels.
Each parent node can have multiple child nodes.
Commonly used in older database systems.

Network Data Model:
Extends the hierarchical model by allowing nodes to have multiple parents (owners).
Uses set theory to represent complex relationships.
Provides more flexibility compared to the hierarchical model.

Object-Oriented Data Model:
Extends the concepts of object-oriented programming to databases.
Represents data as objects with attributes and methods.
Suitable for modeling complex and interconnected data.

Object-Relational Data Model:
Integrates object-oriented features into the relational model.
Supports complex data types, inheritance, and encapsulation.
Offers advantages for handling multimedia and complex data.

NoSQL Data Models:

Various data models, including document, key-value, column-family, and graph models.
Used in NoSQL databases like MongoDB, Redis, Cassandra, and Neo4j.
Designed to handle specific data storage and retrieval requirements.

# Key Components of a Data Model:
Entities: Represent real-world objects or concepts.
Attributes: Describe properties or characteristics of entities.
Relationships: Define connections and associations between entities.
Constraints: Specify rules and limitations on data to maintain integrity.

# Purpose of Data Models:
Facilitate Communication: 
Provides a common language for communication between stakeholders, including designers, developers, and users.
Guide Database Design: 
Serves as a blueprint for structuring databases, guiding the creation of tables, relationships, and constraints.
Ensure Data Integrity: 
Enforces rules and constraints to maintain the accuracy and consistency of data.
Support Documentation: 
Documents the structure and relationships within a database, aiding in understanding and maintenance.



##
Steps in Database Design or Data Modelling:
Database design is a systematic process that involves several steps, from gathering requirements to implementing and maintaining a functional database system. The four major steps in database design are:

Requirements Analysis:
Objective: 
Understand and gather the requirements of the system or application.

Activities:
Conduct interviews with stakeholders.
Identify data entities, relationships, and attributes.
Define business rules and constraints.
Outcome: A clear understanding of what the database is supposed to achieve and the data it needs to store.

# Conceptual Design:
Objective: 
Create a high-level conceptual model that represents the data and its relationships.

Activities:
Use techniques like Entity-Relationship (ER) modeling.
Identify major entities and their relationships.
Develop an ER diagram to illustrate the conceptual model.

Outcome:
A visual representation of entities, attributes, and relationships, serving as a conceptual blueprint for the database.

# Logical Design:
Objective: 
Translate the conceptual model into a logical model that can be implemented in a specific database management system (DBMS).

Activities:
Choose a data model (e.g., relational model).
Specify data types, constraints, and primary keys.
Create a logical schema for tables, views, and indexes.

Outcome: 
A detailed logical model that defines the structure of the database, including tables, fields, relationships, and constraints.

# Physical Design:
Objective: 
Define how the logical model will be implemented in a specific DBMS, considering storage structures and optimization.

Activities:
Specify storage structures (tables, indexes, partitions).
Define primary and foreign key constraints.
Optimize for performance considerations.

Outcome: 
A detailed physical model that addresses the specifics of data storage, indexing, and performance optimization for the chosen DBMS.


##
ERD:

##
Relational Modal:
The relational model is a database model that represents data as tables with rows and columns, where each table is a relation, and each row in the table is a tuple (record) containing attributes (fields or columns). This model was introduced by Edgar F. Codd in 1970, and it has become the dominant data model for database management systems (DBMS).

Customers Table:

+--------------------+-----------------------------+----------------------+
| CustomerID |   CustomerName   |  City		  |
+-------------------+------------------------------+-----------------------+
|    101    	|   John Doe	   	|  NYC   	   |
|    102    	|   Jane Smith  	|  LA    		   |
|    103    	|   Bob Johnson 	|  Chicago 	   |
+--------------------+--------------+--------------+----------------------+


##
Entity:
An entity is a real-world object or concept that has an existence and can be uniquely identified. For example, in a university database, entities could include students, courses, and professors.
Represented as a rectangle in an ERD, with the entity's name inside the rectangle.
    
Entity Type:
An entity type is a collection of entities that share common characteristics or properties. It is a classification or category of entities. For example, "Student" and "Professor" are entity types.

Entity Set:
An entity set is a collection of entities of the same entity type. It represents all instances of a particular entity type at a specific point in time. For instance, the set of all students currently enrolled in a university is an entity set.

Strong Entity:
A strong entity is an entity that has a primary key attribute, and its existence is independent of other entities. It can be uniquely identified by its own attributes.
Represented as a single rectangle. 

Weak Entity:
A weak entity is an entity that does not have a primary key attribute of its own. Its existence is dependent on the existence of another entity, known as the "owning" or "parent" entity. A weak entity is identified by a partial key, which includes a partial set of attributes and relies on the owning entity for identification.
Represented as a double rectangle. A double diamond represents the identifying relationship with the owner entity.

Attributes:
Attributes are properties or characteristics that describe aspects of an entity or relationship. For a "Student" entity, attributes could include "StudentID," "Name," and "DateOfBirth."
Represented as ovals connected to the entity rectangle. Attributes are listed inside these ovals.


Domain:
A domain is the set of all possible values that an attribute can take. For example, the domain of the "DateOfBirth" attribute might be all valid dates.


# Types of Attributes:

Single-valued Attribute:
An attribute that holds a single value for each entity.
Represented by an oval connected to the entity rectangle.

Multivalued Attribute:
An attribute that can hold multiple values for each entity.
Represented by a double oval connected to the entity rectangle.

Simple Attribute:
An attribute that cannot be divided into sub-parts.
Represented by a single oval or double oval (for multivalued attributes) connected to the entity rectangle.

Composite Attribute:
An attribute that can be divided into sub-parts, each with its own meaning.
Represented by an oval with subparts connected by lines to the main oval. The subparts may be single or multivalued attributes.

Derived Attribute:
An attribute whose value is derived or calculated based on other attributes.
Represented by a dashed oval connected to the entity rectangle. The derivation method may be indicated.

Prime Attribute:
An attribute that is part of the primary key.
The primary key attributes are usually underlined in ERD.

Non-Prime Attribute:
An attribute that is not part of the primary key.
Non-prime attributes are not underlined in ERD.

Key:
A key is an attribute or combination of attributes that uniquely identifies an entity within an entity set. A key is crucial for distinguishing one entity from another. In a "Student" entity set, the "StudentID" might be a key.
Primary Key:
A primary key is a unique identifier for a record in a table. It uniquely identifies each row, and no two rows can have the same primary key value.

Rules:
Primary key values cannot be NULL.
Each table can have only one primary key.
Purpose: Uniquely identifies each record in a table.

Foreign Key:
A foreign key is a column or set of columns in one table that refers to the primary key in another table. It establishes a relationship between the two tables.

Rules:
Values in the foreign key must match values in the primary key of the referenced table.
Foreign key values can be NULL if the relationship is optional.
Purpose: Maintains referential integrity between related tables.

Candidate Key:
A candidate key is a set of one or more columns that can uniquely identify a record in a table. It is a potential candidate for being the primary key.

Rules:
Candidate keys must be unique.
They must not contain NULL values.
Purpose: Provides alternative choices for the primary key.

Super Key:
A super key is a set of one or more columns that, taken together, can uniquely identify a record in a table. It may include more columns than necessary to form a minimal candidate key.
Example: If a table has columns A, B, and C, then {A, B} and {A, B, C} are both super keys.
Purpose: Represents all possible combinations of columns that uniquely identify records.

